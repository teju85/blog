I"
<h3 id="proposal">Proposal</h3>
<p>Main paper can be found <a href="https://arxiv.org/abs/1909.12223">here</a>.</p>

<ul>
  <li>a new pairnorm layer which normalizes the intermediate embeddings to avoid
oversmoothing (OS)</li>
  <li>allows deeper layers possible for GNNs</li>
  <li>requires no extra learnable parameters (except for one hyper-param)</li>
</ul>

<h3 id="summary">Summary</h3>
<ul>
  <li>deeper GNNs show gradual loss in accuracy due to
    <ul>
      <li>vanishing gradients</li>
      <li>overfitting due to increased learnable params</li>
      <li>OS</li>
    </ul>
  </li>
  <li>OS?
    <ul>
      <li>phenomenon where node embeddings become very similar to each other</li>
      <li>it is a form of laplacian smoothing</li>
      <li>for shallow nets things are fine as the clusters of nodes will correctly get
similar embeddings</li>
      <li>however, for deeper nets, there’ll be inter-cluster mixing (aka node-wise
smoothing)</li>
      <li>also, repeatedly applying convlutions (or laplacian smoothing) washes out
all the signals in the features (feature-wise smoothing)</li>
    </ul>
  </li>
  <li>in order to study the effects of OS, the authors do the following experiment
    <ul>
      <li>take a SGC but strip it out of all transformation layers</li>
      <li>thus there’ll be no effect of overfitting or vanishing gradients</li>
      <li>they plot the following 2 metrics in order to show the OS behavior</li>
      <li>
\[rowdiff(H^{(k)}) = \frac{1}{n^2} \Sigma_{i,j} \Sigma_p (H_{ip}^{(k)} - H_{jp}^{(k)})^2\]
      </li>
      <li>
\[coldiff(H^{(k)}) = \frac{1}{d^2} \Sigma_{i,j} \Sigma_p (\frac{H_{pi}}{\Sigma_q abs(H_{qi})} - \frac{H_{pj}}{\Sigma_q abs(H_{qj})})^2\]
      </li>
      <li>\(n\) = number of samples</li>
      <li>\(d\) = feature dimension</li>
      <li>\(H^{(k)}\) = computed embedding at ‘k’th hop</li>
    </ul>
  </li>
  <li>authors then show the similarity of GNNs with Graph Regularized Least Squares
(GRLS) method</li>
  <li>then extend GRLS loss function by adding a penalty term against inter-cluster
mixing, in order to minimize the effect of oversmoothing</li>
  <li>they propose pairnorm to maintain the Total Pairwise Squared Distance (TPSD)
metric</li>
  <li>pairnorm
    <ul>
      <li>
\[x_{ik}^c = x_{ik} - \frac{\Sigma_i \Sigma_k x{ik}}{n}\]
      </li>
      <li>
\[x'_{ik} = \frac{s x_{ik}^c}{\sqrt{\frac{1}{n} \Sigma_j \Sigma_p (x_{jp}^c)^2}}\]
      </li>
      <li>\(s\) = hyper-param controlling TPSD</li>
      <li>works well for SGC</li>
      <li>similar to batch-norm layer but without the final scaling and bias</li>
    </ul>
  </li>
  <li>pairnorm-SI (scale individual)
    <ul>
      <li>
\[x'_{ik} = \frac{s x_{ik}^c}{\Sigma_p (x_{ip}^c)^2}\]
      </li>
      <li>works well for SGC, GAT and GCN</li>
    </ul>
  </li>
</ul>
:ET