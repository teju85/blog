I"A<h3 id="mle-and-kld">MLE and KLD</h3>
<ul>
  <li>Assume that \(P_r(x)\) is the real distribution to be approximated</li>
  <li>MLE = \(max_\Theta \frac{1}{m} \Sigma_{i=1}^m log P_\Theta(x_i)\)</li>
  <li>in the limit \(m -&gt; \infty\), this is the same as minimizing KLD!</li>
</ul>

<h3 id="equations">Equations</h3>
<ul>
  <li>
\[lt_{m -&gt; \infty} max_\Theta \frac{1}{m} \Sigma_{i=1}^m log P_\Theta(x)\]
  </li>
  <li>
\[max_\Theta (integral(P_r(x) log P_\Theta(x) dx))\]
  </li>
  <li>
\[min_\Theta (-integral(P_r(x) log P_\Theta(x) dx))\]
  </li>
  <li>
\[min_\Theta (integral(P_r(x) log P_r(x) dx) - integral(P_\Theta(x) log P_\Theta(x) dx))\]
  </li>
  <li>
\[min_\Theta (integral(P_r(x) log \frac{P_r(x)}{P_\Theta(x)} dx))\]
  </li>
  <li>Which is the same as minimizing KLD!</li>
</ul>

<h3 id="issues-with-kld">Issues with KLD</h3>
<ul>
  <li>KLD has numerical stability issues when \(P_r(x)\) or \(P_\Theta(x)\) is close
to zero</li>
  <li>This is typically solved by adding noise to \(P_\Theta\)</li>
  <li>Sampling from \(P_\Theta\) is computationally expensive</li>
</ul>

<h3 id="conclusion">Conclusion</h3>
<ul>
  <li>Thus, better to learn a function \(g_\Theta\) which can transform a given
transform a given distribution into \(P_\Theta\) ie. \(P_\Theta ~= g_\Theta(z)\)</li>
  <li>This is the basis of GANâ€™s :)</li>
</ul>
:ET