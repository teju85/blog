I"µ
<h3 id="proposal">Proposal</h3>
<p>Main paper can be found <a href="https://arxiv.org/pdf/2103.03330v1.pdf">here</a>.</p>

<ul>
  <li>zero-copy based, SM-initiated host-to-device copies of embeddings</li>
  <li>perform aligned reads and MPS (for resource provisioning) to maximize PCIe utilization</li>
</ul>

<h3 id="summary">Summary</h3>
<ul>
  <li>DMA-based host-to-device copies work only when the data is bulk. Embeddings in
GNNâ€™s do not yield well to DMA-based copies. Thus, have SMâ€™s on GPUâ€™s issue
host-to-device from zero-copy buffers</li>
  <li>even while doing zero-copy from the GPUâ€™s care needs to be taken in order to coalesce
and also make sure that the number of PCIe requests are minimized. (Because each PCIe
packet has 12-16B header overhead)</li>
  <li>need to also fully utilize the PCIe links by issuing as many such requests as
possible. On PCIe3 number of such outstanding requests is 256 and 768 on PCIe4.</li>
  <li>So, to reduce the number of PCIe requests due to misaligned addresses, a circular
shift is applied to all threads and warps.</li>
  <li>As expected, this circular shift is not necessary if the embedding size is less
than the GPU cacheline size or it is already 128B aligned.</li>
  <li>We also need to make sure that the computation kernels are running alongside of
this zero-copy kernel in order to maximize GPU utilization.</li>
  <li>But this is not guaranteed when there are blocking CUDA API calls like
<code class="language-plaintext highlighter-rouge">cudaMalloc</code>, <code class="language-plaintext highlighter-rouge">cudaFree</code>, etc.</li>
  <li>So, the solution is to use MPS to provision the amount of SMâ€™s to be dedicated
for the purpose of zero-copy. Thus, thereâ€™ll be atleast 2 processes: one for
zero-copy and the other for compute and CUDA IPC will be used to share the
buffer addresses between these.</li>
  <li>The sample -&gt; gather -&gt; compute flow is pipelined (using ping-pong buffers) to
maximize overlap of these phases.</li>
  <li>For multi-GPU training, this whole setup is duplicated across every GPU by just
relying on DGLâ€™s data-parallel approach. The catch though is that unified memory
in CUDA is not shareable across processes! So, they first create a host shared
memory region and then call <code class="language-plaintext highlighter-rouge">cudaHostRegister</code> on this buffer inside each
process for each of the GPU in the current node.</li>
  <li>They note that atleast on RTX-3090, SM split of 15:85 between zero-copy and the
actual training kernels is good enough to saturate PCIe bandwidth.</li>
  <li>However they also note that a 10:90 split didnâ€™t show any noticeable change in
the overall end-to-time minibatch time</li>
</ul>
:ET