I"õ<h1 id="nvshmem">NVSHMEM</h1>

<h2 id="intro">Intro</h2>
<p>API document can be found <a href="https://docs.nvidia.com/hpc-sdk/nvshmem/api/docs/index.html">here</a>
and developer guide can be found <a href="https://docs.nvidia.com/hpc-sdk/nvshmem/developer-guide/index.html">here</a>.</p>

<p>nvshmem is an extension of openshmem API for the GPU clusters. nvshmem provides
a PGAS (Partitioned Global Address Space) for the buffers on GPU cluster.
Enables fine-grained computation/communication overlap and also performing
synchronization, all from the CUDA kernels itself. Thus, this feature is very
helpful in achieving strong scaling on our apps. However, it also provides
CPU-side APIs to initiate communication, for flexibility.</p>

<h2 id="programming-model">Programming model</h2>
<p>Types of data objects</p>
<ol>
  <li>Symmetric data objects - objects can be shared across remote PEâ€™s. These are
allocated from a <strong>symmetric heap</strong> using <code class="language-plaintext highlighter-rouge">nvshmem_malloc()</code>.</li>
  <li>Private data objects - objects that private to the PE which owns them</li>
</ol>

<p>openshmem provides strong ordering guarantees. However, for perf reasons,
nvshmem doesnâ€™t provide this guarantee and thus it is expected of the devs to
use <code class="language-plaintext highlighter-rouge">nvshmem_fence()</code> when such an ordering is required! But, non-blocking calls
are not ordered with this call and for those, we need to use <code class="language-plaintext highlighter-rouge">nvshmem_quiet()</code>.</p>

<p>Execution model is typically SPMD (Single Program Multiple Data), but this is
not required by nvshmem. Work is done by PEs, which are typically OS processes.
These PEs are further allowed to create threads, if thereâ€™s a support for it.
nvshmem phase begins with the call to either <code class="language-plaintext highlighter-rouge">nvshmem_init()</code> or
<code class="language-plaintext highlighter-rouge">nvshmem_init_thread()</code> and concludes with the call to <code class="language-plaintext highlighter-rouge">nvshmem_finalize()</code>
done by all PEs or <code class="language-plaintext highlighter-rouge">nvshmem_global_exit()</code> by any PE. We cannot reinitialize the
nvshmem after finalization! (similar to MPI) Each PE has an integer ID, similar
to the concept of MPI ranks.</p>

<p>Since nvshmem provides one-sided communication APIâ€™s, even if the target PE is
not involved in any nvshmem calls, other PEâ€™s can continue to communicate with
this PE and make progress. This is unlike MPI communication model.</p>

<p>Thereâ€™s also threadgroup communication with multiple threads can collectively be
part of a single communication operation.</p>

<p>nvshmem expects all buffer arguments to nvhsmem communication routines to be
symmetric objects.</p>

<p>symmetric address returned by nvhsmem allocation routine is also a valid local
addresses. However, trying to use a mix of symmetric address and local address
to nvshmem routines can lead to undefined behavior.</p>

<h2 id="communication-model">Communication model</h2>
<p>Provides <strong>get</strong> and <strong>put</strong> methods for working with symmetric objects. Has all
the bulk, scalar and interleaved transfer schemes available. Also supports
atomic memory operations (AMO). There are methods can be initiated from CUDA
kernels or from the host too. All the CPU-side calls are stream-ordered.</p>

<p><code class="language-plaintext highlighter-rouge">nvshmem_ptr()</code> provides raw address pointer to be used for issuing explicit
loads/stores to local or remote PEs, as long as they are accessible to each
other.</p>

<p>All nvshmem symmmetric memory is pinned GPU memory.</p>

<p>nvshmem assumes a one-to-one mapping between PE and GPU.</p>

<p>The usual data coalescing policies/guidelines as seen in CUDA programming model
also apply here for efficiency.</p>

<p>cuda kernels needing sync/collective APIs must be launched using the collective
launch APIs only.</p>

<h2 id="memory-model">memory model</h2>
<p>List of operations supported:</p>
<ul>
  <li>Remote Memory Access (RMA) - PUT/GET</li>
  <li>Atomic Memory Operations (AMO)</li>
  <li>single ops</li>
  <li>direct loads/stores (via the pointer returned by <code class="language-plaintext highlighter-rouge">nvshmem_ptr()</code></li>
  <li>collective ops</li>
  <li>wait and test functions</li>
</ul>

<p>Since GPUs expose only a weak memory model, nvshmem does introduce a few
exceptions to the openshmem memory model</p>
<ul>
  <li>no guarantees to the order of writes to a symmetric memory, as seen by PEs</li>
  <li>to enforce ordering to target PE use <code class="language-plaintext highlighter-rouge">nvshmem_fence()</code></li>
  <li>to enforce ordering to all other PEâ€™s use <code class="language-plaintext highlighter-rouge">nvshmem_quiet()</code></li>
  <li>to enforce ordering for nonblocking calls use <code class="language-plaintext highlighter-rouge">nvshmem_quiet()</code></li>
</ul>

<p>Result of the get or AMO operations will appear in the program order, however.</p>
:ET