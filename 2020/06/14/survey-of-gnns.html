<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="stylesheet" href="/blog/assets/main.css"> <link rel="stylesheet" href="/blog/assets/katex.min.css"> <script rel="src" src="/blog/assets/jquery-1.11.1.min.js"></script> <script rel="src" src="/blog/assets/katex.min.js"></script> </head> <body> <header class="site-header" role="banner"> <div class="wrapper"> <a class="site-title" rel="author" href="/blog/">Quagmire</a> <nav class="site-nav"> <input type="checkbox" id="nav-trigger" class="nav-trigger" /> <label for="nav-trigger"> <span class="menu-icon"> <svg viewBox="0 0 18 15" width="18px" height="15px"> <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/> <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/> <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/> </svg> </span> </label> <div class="trigger"> <a class="page-link" href="/blog/about/">About</a> <a class="page-link" href="/blog/allposts/">All Posts</a> <a class="page-link" href="/blog/tags.html">Tags</a> </div> </nav> </div> </header> <main class="page-content" aria-label="Content"> <div class="wrapper"> <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting"> <header class="post-header"> <h1 class="post-title p-name" itemprop="name headline">A comprehensive survey on Graph Neural Networks</h1> <p class="post-meta"> <time class="dt-published" datetime="2020-06-14T00:00:00+05:30" itemprop="datePublished"> Jun 14, 2020 </time> </p> </header> <ul class="tags"> <li><a href="/blog/tags.html#tech-notes" class="tag">tech-notes</a></li> <li><a href="/blog/tags.html#paper-notes" class="tag">paper-notes</a></li> <li><a href="/blog/tags.html#graph" class="tag">graph</a></li> <li><a href="/blog/tags.html#gnn" class="tag">gnn</a></li> </ul> <div class="post-content e-content" itemprop="articleBody"> <h3 id="proposal">Proposal</h3> <p>Main survey paper can be found <a href="https://arxiv.org/pdf/1901.00596.pdf">here</a>. Divides GNNs into the following top-level four categories:</p> <ol> <li>recurrent GNNs - iteratively exchange info with neighbors until some form of convergence is achieved. This idea is borrowed and improved by convGNNs.</li> <li>convolutional GNNs - same as recGNNs, but here multiple such operations are stacked to extract high-level node representations based on neighbors that are further away.</li> <li>graph autoencoders - unsupervised methods to represent graphs by latent vectors and then use a decoder phase to reconstruct the graph</li> <li>spatio-temporal GNNs - combine convGNNs and then regular 1D-CNNs to capture both spatial and temporal info of the nodes, respectively</li> </ol> <h3 id="differences-wrt-regular-dl">Differences wrt regular DL</h3> <ul> <li>data space is non-Euclidean</li> <li>graphs can be irregular</li> <li>number of neighbors vary, causing difficulty with regular convolutions</li> <li>instance (aka node) is not independent of the other</li> </ul> <h3 id="differences-wrt-network-embedding">Differences wrt network embedding</h3> <ul> <li>NE involves other non-DL methods like factorization, random-walks</li> <li>GNNs provide and end-to-end solution for some of the graph-related tasks</li> </ul> <h3 id="recgnns">recGNNs</h3> <p>Perform message passing between nodes and their neighbors, until some form of convergence is achieved. <script type="math/tex">h_v^{(t)} = \sum_{u \epsilon N(v)} f(X_v, x_{u,v}^e, x_u, h_u^{(t-1)})</script></p> <ul> <li><script type="math/tex">h_v</script> = hidden vector which is initialized to random value at time = 0</li> <li><script type="math/tex">f(.)</script> = a contraction mapping parametric function to ensure convergence</li> <li><script type="math/tex">X</script> = node feature matrix</li> <li><script type="math/tex">x</script> = edge feature matrix</li> <li><script type="math/tex">N(v)</script> = neighborhood of node <script type="math/tex">v</script></li> <li>n = number of nodes</li> <li>m = number of edges</li> </ul> <p>Gated GNNs - use GRU with fixed number of recurrence steps, instead of a generic contraction mapping function. Then uses BPTT (backprop through time) in order to learn parameters. <script type="math/tex">h_v^{(t)} = GRU(h_v^{(t-1)}, \sum_{u \epsilon N(v)} W h_u^{(t-1)}</script></p> <ul> <li> <script type="math/tex; mode=display">h_v^{(0)} = x_v</script> </li> </ul> <h3 id="convgnns">convGNNs</h3> <h4 id="spectral-based">spectral based</h4> <p>Based on graph signal processing and use normalized graph laplacian <script type="math/tex">L = I - D^{-0.5} A D^{-0.5}</script></p> <ul> <li><script type="math/tex">D</script> = diagonal matrix with <script type="math/tex">D_{ii} = \sum_j A_{ij}</script></li> </ul> <p><script type="math/tex">L</script> is real symmetric and positive semidefinite. So, it can be factored into <script type="math/tex">L = U \Lambda U^T</script></p> <ul> <li><script type="math/tex">U</script> = columwise eigenvectors ordered by eigenvalues</li> <li><script type="math/tex">\Lambda</script> = diagonal matrix of eigenvalues</li> </ul> <p>Graph fourier transform on input <script type="math/tex">x \epsilon R^n</script> is defined as <script type="math/tex">y = U^T x</script> similarly vice-versa for graph inverse fourier transform.</p> <p>Assuming a filter <script type="math/tex">g \epsilon R^n</script>, convolution operation will become <script type="math/tex">x*g = U (U^T x \odot U^T g) = U g_{\theta} U^T x</script> with <script type="math/tex">g_{\theta} = U^T g</script> All spectral methods follow this, except for their choice of <script type="math/tex">g_{\theta}</script>. Some methods are:</p> <ol> <li>Spectral CNNs</li> <li>Chebyshev spectral CNNs (ChebNet)</li> <li>CayleyNet</li> <li><a href="/blog/2020/06/09/graph-convolutional-networks.html">GCN</a> these are however a mix of spectral and spatial based by approximating ChebNet at first-order and simplifying learnable parameters.</li> </ol> <h4 id="spatial-based">spatial based</h4> <h4 id="spectral-vs-spatial">spectral vs spatial</h4> <ul> <li>spectral methods have theoretical framework</li> <li>spatial methods are computationally efficient and scalable</li> <li>spatial methods allow batching of nodes in a graph</li> <li>spectral methods only work with undirected graphs</li> </ul> <h4 id="graph-pooling-modules">graph pooling modules</h4> <h3 id="graph-autoencoders">Graph autoencoders</h3> <h4 id="network-embedding">network embedding</h4> <h4 id="graph-generation">graph generation</h4> <h3 id="spatio-temporal-gnns">spatio-temporal GNNs</h3> <h3 id="future-directions">future directions</h3> <ol> <li>model depth - going infinite depth will pull all nodes into a single point! Does it make sense to increase depth?</li> <li>scalability trade-off - use of clustering could destroy patterns, while sampling the neighbors may miss critical info at times. Need to find a good trade-off between scalability and info-loss</li> <li>heterogeneous graphs support</li> <li>dynamicity - STGNNs addresses some aspects of this.</li> </ol> </div> <a class="u-url" href="/blog/2020/06/14/survey-of-gnns.html" hidden></a> </article> </div> </main> <script type="text/javascript"> $("script[type='math/tex']").replaceWith( function(){ var tex = $(this).text(); return "<span class=\"inline-equation\">" + katex.renderToString(tex) + "</span>"; }); $("script[type='math/tex; mode=display']").replaceWith( function(){ var tex = $(this).text(); return "<div class=\"equation\">" + katex.renderToString("\\displaystyle "+tex) + "</div>"; }); </script> <footer class="site-footer h-card"> <data class="u-url" href="/blog/"></data> <div class="wrapper"> <div class="footer-col-wrapper"> <div class="footer-col footer-col-3"> <p>Stuff I find cool/useful</p> </div> </div> </div> </footer> </body> </html>
