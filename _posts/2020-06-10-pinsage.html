<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/blog/src/assets/main.css">

  <link rel="stylesheet" href="/blog/src/assets/katex.min.css">
  <script rel="src" src="/blog/src/assets/jquery-1.11.1.min.js"></script>
  <script rel="src" src="/blog/src/assets/katex.min.js"></script>
  <script rel="src" src="/blog/src/assets/auto-render.min.js"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
              {left: '$$', right: '$$', display: false},
              {left: '$', right: '$', display: false}
          ],
          throwOnError : false
        });
    });
  </script>


</head>
  <body>
    <header class="site-header" role="banner">
  <div class="wrapper">
    
    
    <a class="site-title" rel="author" href="/blog/">Quagmire</a>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
            <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
            <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
          </svg>
        </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/blog/about.html">About</a>
        <a class="page-link" href="/blog/allposts.html">All Posts</a>
        <a class="page-link" href="/blog/tags.html">Tags</a>
      </div>
    </nav>
  </div>
</header>
    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">PinSage</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-06-10" itemprop="datePublished">
        Published: 2020-06-10
      </time>
      
        <br/>
        <time class="dt-published" datetime="2021-07-15" itemprop="datePublished">
          Last Modifed: 2021-07-15
        </time>
      
    </p>
  </header>

  <ul class="tags">
  
    
    <li><a href="/blog/tags.html#tech-notes" class="tag">tech-notes</a></li>
  
    
    <li><a href="/blog/tags.html#paper-notes" class="tag">paper-notes</a></li>
  
    
    <li><a href="/blog/tags.html#graph" class="tag">graph</a></li>
  
    
    <li><a href="/blog/tags.html#gnn" class="tag">gnn</a></li>
  
  </ul>

  <div class="post-content e-content" itemprop="articleBody">
    <h3>PinSage - Graph Convolutional Neural Networks for Web-ScaleRecommender Systems</h3>
<p>Main paper can be found <a href="https://arxiv.org/pdf/1806.01973.pdf">here</a> and a more
digestable version is presented in a blog format <a href="https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48">here</a>.
A scalable random-walk GCN to learn embeddings on attribute graphs with billions
of nodes. They demonstrate this on Pinterest&#x27;s data with billions of nodes and
tens of billions of edges. They also present an efficient way to do inferencing
on the resulting model via MapReduce paradigm.</p>
<h4>Proposal</h4>
<p>For improving scalability:</p>
<ol>
<li>on-the-fly convolutions: sampling the neighborhood to dynamically construct
the computation graph</li>
<li>producer-consumer minibatch construction: CPU producing minibatches via
efficiently sampling the neighborhood GPU performs SGD.</li>
<li>efficient MapReduce based inference
For improving accuracy:</li>
<li>constructing convs via random walks: sampling of neighbors is done via random
walks on the graph. This also helps provide weightage during aggregation.</li>
<li>importance pooling - same as above</li>
<li>Curriculum training</li>
</ol>
<h4>Details</h4>
<p>Current GCNs expect the availability of full graph Laplacian during training.
This is very infeasible for web-scale graphs with billions of nodes and edges.</p>
<p><strong>Inputs</strong></p>
<ul>
<li>G = Graph in CSR representation</li>
<li>$$x_u$$ = node features for each $$u \epsilon G$$</li>
<li>d = dimensionality of the features. This is also the dimensionality of the
output of each layer.</li>
<li>L = labelled pairs $$(q, i)$$, where $$i$$ represents a good recommendation
if the input query is $$q$$</li>
</ul>
<p><strong>Hyper-params</strong></p>
<ul>
<li>m = dimensionality of the output vector of the aggregation step. This is
assumed to be the same value across all layers</li>
<li>K = number of layers</li>
<li>$$\Delta$$ = max-margin used in the loss function</li>
<li>B = batch size</li>
<li>T = max number of neighbors to be sampled for any node</li>
</ul>
<p><strong>Model</strong></p>
<ul>
<li>$$Q^k$$ = neighborhood feature transformation matrix for $$convolve_k$$ at kth
layer of the network. Dim = m x d</li>
<li>$$q^k$$ = bias vector for this operation at kth layer. Length = m</li>
<li>$$W^k$$ = feature transformation matrix after aggregation at kth layer.
Dim = d x (d + m)</li>
<li>$$w^k$$ = bias vector for this operation at kth layer. Length = d</li>
<li>$$G_1$$ = 1st feature transformation matrix at final layer. Dim = d x d</li>
<li>$$g$$ = bias vector for this operation. Length = d</li>
<li>$$G_2$$ = 2nd feature transformation at final layer. Dim = d x d</li>
</ul>
<p><strong>Local convolutions algorithm at kth layer</strong></p>
<p>$$n_u = \gamma(relu(Q^k h_v^k + q^k), \alpha) | v \epsilon N(u)$$</p>
<p>$$m_u = concat(h_u^k, n_u)$$</p>
<p>$$h_u^{k+1} = relu(W^k m_u + w^k)$$</p>
<p>$$h_u^{k+1} = \frac{h_u^{k+1}}{||h_u^{k+1}||_2}$$</p>
<ul>
<li>$$\alpha$$ = set of neighbor weights computed based on random-walk. These are
computed as $$L_1$$ norm of visit counts of each node during random-walk.</li>
<li>$$\gamma$$ = aggregation function. Assumed to be weighted-mean</li>
<li>$$N(u)$$ = sampled neighborhood for node u</li>
</ul>
<p><strong>minibatch preparation</strong></p>
<p>Happens on CPU for the next batch while current batch is running on GPU. Uses
openmp for parallelization.</p>
<p><strong>Model propagation</strong></p>
<ul>
<li>Compute the neighbors for each layer starting from the topmost K-th layer</li>
<li>Compute the K-layers of convolve operation</li>
<li>Finally apply $$G_1$$ and $$G_2$$ transformations</li>
</ul>
<p><strong>Loss function</strong></p>
<p>Max-margin ranking loss. Dot product of the embeddings of negative samples is
atleast $$\Delta$$ distance lesser than that of $$(q, i)$$. In other words:</p>
<p>$$ loss(q, i) = max(0, z_q . z_{n_k} - z_q . z_i + \Delta)$$</p>
<ul>
<li>$$n_k$$ = negative sample for query $$q$$</li>
</ul>
<p><strong>training</strong></p>
<p>Multi-GPU training. Each GPU gets equal amount from the minibatch. Standard
synchronous SGD is used with the following learning rate policy:</p>
<ol>
<li>At first epoch, learning rate is linearly increased from a small value to the
peak value.</li>
<li>At subsequent epochs, learning rate is exponentially decayed.</li>
</ol>
<p><strong>negative sampling</strong></p>
<p>Each positive training example $$(q, i)$$ contains multiple &quot;hard&quot; negative
samples too. These hard negative samples are generated via personalized
pagerank scores. To reduce the number of epochs, curriculum training is being
used. At n-th epoch, n - 1 hard negative samples are added for each positive
training sample.</p>

  </div>
  <a class="u-url" href="/blog/_posts/2020-06-10-pinsage.html" hidden></a>
</article>
      </div>
    </main>
    <footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-3">
        <p>Stuff I find cool/useful</p>
      </div>
    </div>
  </div>
</footer>
  </body>
</html>