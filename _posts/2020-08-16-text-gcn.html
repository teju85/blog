<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/src/assets/main.css">
  
    <link rel="stylesheet" href="/src/assets/katex.min.css">
    <script rel="src" src="/src/assets/jquery-1.11.1.min.js"></script>
    <script rel="src" src="/src/assets/katex.min.js"></script>
  
  
</head>
  <body>
    <header class="site-header" role="banner">
  <div class="wrapper">
    
    
    <a class="site-title" rel="author" href="/">Quagmire</a>

    
  </div>
</header>
    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">"Graph Convolutional Networks for text classification"</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-08-16" itemprop="datePublished">
        Published: 2020-08-16
      </time>
      <time class="dt-published" datetime="2021-07-15" itemprop="datePublished">
        Last Modifed: 2021-07-15
      </time>
    </p>
  </header>

  <ul class="tags">
  
    <li><a href="/blog/tags.html#tech-notes" class="tag">tech-notes</a></li>
  
    <li><a href="/blog/tags.html#paper-notes" class="tag">paper-notes</a></li>
  
    <li><a href="/blog/tags.html#graph" class="tag">graph</a></li>
  
    <li><a href="/blog/tags.html#gnn" class="tag">gnn</a></li>
  
  </ul>

  <div class="post-content e-content" itemprop="articleBody">
    <h3>Proposal</h3>
<p>Main paper can be found <a href="https://arxiv.org/pdf/1809.05679.pdf">here</a>.</p>
<ul>
<li>novel GNN based method for text classification</li>
<li>graph is constructed using word co-occurence and word-doc associations</li>
<li>learn both word and doc (aka corpus) embeddings simultaneously by building and
using the above mentioned heterogenous text graph, where nodes are both words
and docs</li>
</ul>
<h3>Summary</h3>
<ul>
<li>text graph construction
<ul>
<li>based on word co-occurence (in corpus) and doc-word relations</li>
<li>number of nodes = vocab size + corpus size</li>
<li>input embeddings are just one-hot vectors</li>
<li>edge weights
<ul>
<li>doc to word = TFIDF</li>
<li>word to word = $$PMI(i, j)$$ only if $$PMI(i, j)$$ is positive and words
i and j occur in the sliding window together</li>
<li>self-loops are added</li>
<li>$$PMI(i, j) = log\frac{P_{ij}}{P_i P_j}$$</li>
<li>$$P_{ij} = \frac{nW(i, j)}{nW}$$</li>
<li>$$P(i) = \frac{nW(i)}{nW}$$</li>
<li>$$nW$$ = total number of sliding windows</li>
</ul>
</li>
</ul>
</li>
<li>model and training
<ul>
<li>usage of spectral GCNs as in Kipf and Welling</li>
<li>2 layer GCNs</li>
<li>output layer is psased through softmax classifier</li>
<li>loss function is cross entropy error over all labelled docs
<ul>
<li>$$L = - \Sigma_d \Sigma_f Y_{df} ln(Z_{df})$$</li>
<li>$$d$$ = all labelled docs</li>
<li>$$f$$ = all output features</li>
<li>$$Y$$ = label vector for a doc</li>
<li>$$Z$$ = softmax output vector</li>
</ul>
</li>
<li>2 layers help information exchange between docs too!</li>
<li>more layers didn&#x27;t help improving accuracy</li>
<li>sliding window size = 20 words</li>
<li>first layer output embedding size = 200</li>
<li>learning rate = 0.02</li>
<li>Adam optimizer with 200 epochs</li>
<li>early stopping after 10 epochs with no decrease in validation loss</li>
<li>dropout rate = 0.5</li>
<li>10% of training as validation set</li>
</ul>
</li>
</ul>

  </div>
  <a class="u-url" href="./_posts/2020-08-16-text-gcn.html" hidden></a>
</article>
      </div>
    </main>
    
  <script type="text/javascript">
   $("script[type='math/tex']").replaceWith(
     function(){
       var tex = $(this).text();
       return "<span class=\"inline-equation\">" + 
              katex.renderToString(tex) +
              "</span>";
   });
   
   $("script[type='math/tex; mode=display']").replaceWith(
     function(){
       var tex = $(this).text();
       return "<div class=\"equation\">" + 
              katex.renderToString("\\displaystyle "+tex) +
              "</div>";
   });
  </script>
      

<footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-3">
        <p>Stuff I find cool/useful</p>
      </div>
    </div>
  </div>
</footer>
  </body>
</html>