<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/src/assets/main.css">
  
    <link rel="stylesheet" href="/src/assets/katex.min.css">
    <script rel="src" src="/src/assets/jquery-1.11.1.min.js"></script>
    <script rel="src" src="/src/assets/katex.min.js"></script>
  
  
</head>
  <body>
    <header class="site-header" role="banner">
  <div class="wrapper">
    
    
    <a class="site-title" rel="author" href="/">Quagmire</a>

    
  </div>
</header>
    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">"GraphSAGE"</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-06-24" itemprop="datePublished">
        Published: 2020-06-24
      </time>
      <time class="dt-published" datetime="2021-07-15" itemprop="datePublished">
        Last Modifed: 2021-07-15
      </time>
    </p>
  </header>

  <ul class="tags">
  
    <li><a href="/blog/tags.html#tech-notes" class="tag">tech-notes</a></li>
  
    <li><a href="/blog/tags.html#paper-notes" class="tag">paper-notes</a></li>
  
    <li><a href="/blog/tags.html#graph" class="tag">graph</a></li>
  
    <li><a href="/blog/tags.html#gnn" class="tag">gnn</a></li>
  
  </ul>

  <div class="post-content e-content" itemprop="articleBody">
    <h3>Proposal</h3>
<p>Main paper can be found <a href="https://arxiv.org/pdf/1706.02216.pdf">here</a>.
SaGe = Sample and Aggregation functions</p>
<ul>
<li>Learns parametric functions in order to generate low dimensional embeddings of
the nodes in the graph</li>
<li>These functions can also be used to generate embeddings of unseen nodes too</li>
<li>It can also optionally use node features as starting points for embeddings</li>
<li>Configurable number of layers representing the number of hops for each node</li>
<li>Custom loss function (based on negative-sampling) for unsupervised learning</li>
<li>Also supports supervised learning</li>
<li>Scales to large graphs through neighborhood sampling and minibatch techniques</li>
</ul>
<h3>forward propagation algo</h3>
<ul>
<li>$$h_v^0 = x_v$$</li>
<li>for k = 1 to K
<ul>
<li>for v in minibatch
<ul>
<li>$$h_{N(v)}^k = aggregator_k(h_u^{k-1}, u \epsilon N(v))$$</li>
<li>$$h_v^k = \sigma(W^k concat(h_v^{k-1}, h_{N(v)}^k))$$</li>
<li>$$h_v^k = \frac{h_v^k}{L2norm(h_v^k)}$$</li>
</ul>
</li>
</ul>
</li>
<li>$$z_v = h_v^K$$</li>
</ul>
<p>Where:</p>
<ul>
<li>$$h^k$$ = hidden vectors at each layer for all vertices</li>
<li>$$V$$ = nodes</li>
<li>$$N(v)$$ = sampled neighborhood for each node</li>
<li>$$K$$ = number of layers (hyperparameter)</li>
<li>$$\sigma$$ = a non-linear activation function</li>
<li>$$W^k$$ = learnable weights for each layer</li>
<li>$$z$$ = output embedding</li>
</ul>
<p>In paper the authors show the similarity of this algo with the popular
<a href="https://www.davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/">Weisfeiler-Lehman</a>
graph isomorphism test.</p>
<h3>neighborhood sampling</h3>
<ul>
<li>performs uniform sampling</li>
<li>different samples are drawn for each layer based on different sampling rates
$$S_i$$, for each layer</li>
<li>authors recommend to keep $$\Pi_i S_i \le 500$$</li>
<li>sampling technique is pretty much the same as seen in
[PinSage]({{ site.baseurl }}{% post_url 2020-06-10-pinsage %})</li>
<li>They recommend $$K = 2, S_1 = 25, S_2 = 10$$</li>
</ul>
<h3>learning</h3>
<ul>
<li>via SGD + using Adam optimizer</li>
<li>loss function for unsupervised mode is based on negative sampling and its
equation is as follows: $$J(z_u) = -log(sigmoid(z_u^T z_v)) - Q E[log(sigmoid(z_u^T z_{v_n}))]$$</li>
<li>$$Q$$ = number of negative samples</li>
<li>$$v_n$$ = negative sample</li>
<li>$$v$$ = node that is a neighbor of $$u$$</li>
</ul>
<h3>aggregators</h3>
<p>These must be permutation invariant. They test the following aggregators</p>
<h4>mean aggregator</h4>
<ul>
<li>$$h_v^k = \sigma(W^k Mean(h_u^{k-1} with u \epsilon N(v), h_v^{k-1}))$$</li>
<li>note that this uses $$h_v^{k-1}$$ in the aggregation itself!</li>
<li>thus this can be thought of as &quot;skip connection&quot; between layers</li>
</ul>
<h4>LSTM aggregator</h4>
<ul>
<li>LSTM&#x27;s are not permutation invariant</li>
<li>So they make it to be invariant by randomly permuting the inputs</li>
<li>this seems to show the best accuracy in their experimentation</li>
</ul>
<h4>pooling aggregator</h4>
<ul>
<li>$$h_{N(v)}^k = max_{u \epsilon N(v)}(\sigma(W_{pool}^k h_u^k + b^k))$$</li>
<li>aggregator can be either max or mean, both seem to provide similar accuracy</li>
</ul>

  </div>
  <a class="u-url" href="./_posts/2020-06-24-graphsage.html" hidden></a>
</article>
      </div>
    </main>
    
  <script type="text/javascript">
   $("script[type='math/tex']").replaceWith(
     function(){
       var tex = $(this).text();
       return "<span class=\"inline-equation\">" + 
              katex.renderToString(tex) +
              "</span>";
   });
   
   $("script[type='math/tex; mode=display']").replaceWith(
     function(){
       var tex = $(this).text();
       return "<div class=\"equation\">" + 
              katex.renderToString("\\displaystyle "+tex) +
              "</div>";
   });
  </script>
      

<footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-3">
        <p>Stuff I find cool/useful</p>
      </div>
    </div>
  </div>
</footer>
  </body>
</html>