<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/blog/src/assets/main.css">

  <link rel="stylesheet" href="/blog/src/assets/katex.min.css">
  <script rel="src" src="/blog/src/assets/jquery-1.11.1.min.js"></script>
  <script rel="src" src="/blog/src/assets/katex.min.js"></script>
  <script rel="src" src="/blog/src/assets/auto-render.min.js"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
              {left: '$$', right: '$$', display: false},
              {left: '$', right: '$', display: false}
          ],
          throwOnError : false
        });
    });
  </script>


</head>
  <body>
    <header class="site-header" role="banner">
  <div class="wrapper">
    
    
    <a class="site-title" rel="author" href="/blog/">Quagmire</a>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
            <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
            <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
          </svg>
        </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/blog/about.html">About</a>
        <a class="page-link" href="/blog/allposts.html">All Posts</a>
        <a class="page-link" href="/blog/tags.html">Tags</a>
      </div>
    </nav>
  </div>
</header>
    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">GraphSAGE</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-06-24" itemprop="datePublished">
        Published: 2020-06-24
      </time>
      
        <br/>
        <time class="dt-published" datetime="2021-07-15" itemprop="datePublished">
          Last Modifed: 2021-07-15
        </time>
      
    </p>
  </header>

  <ul class="tags">
  
    
    <li><a href="/blog/tags.html#tech-notes" class="tag">tech-notes</a></li>
  
    
    <li><a href="/blog/tags.html#paper-notes" class="tag">paper-notes</a></li>
  
    
    <li><a href="/blog/tags.html#graph" class="tag">graph</a></li>
  
    
    <li><a href="/blog/tags.html#gnn" class="tag">gnn</a></li>
  
  </ul>

  <div class="post-content e-content" itemprop="articleBody">
    <h3>Proposal</h3>
<p>Main paper can be found <a href="https://arxiv.org/pdf/1706.02216.pdf">here</a>.
SaGe = Sample and Aggregation functions</p>
<ul>
<li>Learns parametric functions in order to generate low dimensional embeddings of
the nodes in the graph</li>
<li>These functions can also be used to generate embeddings of unseen nodes too</li>
<li>It can also optionally use node features as starting points for embeddings</li>
<li>Configurable number of layers representing the number of hops for each node</li>
<li>Custom loss function (based on negative-sampling) for unsupervised learning</li>
<li>Also supports supervised learning</li>
<li>Scales to large graphs through neighborhood sampling and minibatch techniques</li>
</ul>
<h3>forward propagation algo</h3>
<ul>
<li>$$h_v^0 = x_v$$</li>
<li>for k = 1 to K
<ul>
<li>for v in minibatch
<ul>
<li>$$h_{N(v)}^k = aggregator_k(h_u^{k-1}, u \epsilon N(v))$$</li>
<li>$$h_v^k = \sigma(W^k concat(h_v^{k-1}, h_{N(v)}^k))$$</li>
<li>$$h_v^k = \frac{h_v^k}{L2norm(h_v^k)}$$</li>
</ul>
</li>
</ul>
</li>
<li>$$z_v = h_v^K$$</li>
</ul>
<p>Where:</p>
<ul>
<li>$$h^k$$ = hidden vectors at each layer for all vertices</li>
<li>$$V$$ = nodes</li>
<li>$$N(v)$$ = sampled neighborhood for each node</li>
<li>$$K$$ = number of layers (hyperparameter)</li>
<li>$$\sigma$$ = a non-linear activation function</li>
<li>$$W^k$$ = learnable weights for each layer</li>
<li>$$z$$ = output embedding</li>
</ul>
<p>In paper the authors show the similarity of this algo with the popular
<a href="https://www.davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/">Weisfeiler-Lehman</a>
graph isomorphism test.</p>
<h3>neighborhood sampling</h3>
<ul>
<li>performs uniform sampling</li>
<li>different samples are drawn for each layer based on different sampling rates
$$S_i$$, for each layer</li>
<li>authors recommend to keep $$\Pi_i S_i \le 500$$</li>
<li>sampling technique is pretty much the same as seen in
[PinSage]({{ site.baseurl }}{% post_url 2020-06-10-pinsage %})</li>
<li>They recommend $$K = 2, S_1 = 25, S_2 = 10$$</li>
</ul>
<h3>learning</h3>
<ul>
<li>via SGD + using Adam optimizer</li>
<li>loss function for unsupervised mode is based on negative sampling and its
equation is as follows: $$J(z_u) = -log(sigmoid(z_u^T z_v)) - Q E[log(sigmoid(z_u^T z_{v_n}))]$$</li>
<li>$$Q$$ = number of negative samples</li>
<li>$$v_n$$ = negative sample</li>
<li>$$v$$ = node that is a neighbor of $$u$$</li>
</ul>
<h3>aggregators</h3>
<p>These must be permutation invariant. They test the following aggregators</p>
<h4>mean aggregator</h4>
<ul>
<li>$$h_v^k = \sigma(W^k Mean(h_u^{k-1} with u \epsilon N(v), h_v^{k-1}))$$</li>
<li>note that this uses $$h_v^{k-1}$$ in the aggregation itself!</li>
<li>thus this can be thought of as &quot;skip connection&quot; between layers</li>
</ul>
<h4>LSTM aggregator</h4>
<ul>
<li>LSTM&#x27;s are not permutation invariant</li>
<li>So they make it to be invariant by randomly permuting the inputs</li>
<li>this seems to show the best accuracy in their experimentation</li>
</ul>
<h4>pooling aggregator</h4>
<ul>
<li>$$h_{N(v)}^k = max_{u \epsilon N(v)}(\sigma(W_{pool}^k h_u^k + b^k))$$</li>
<li>aggregator can be either max or mean, both seem to provide similar accuracy</li>
</ul>

  </div>
  <a class="u-url" href="/blog/_posts/2020-06-24-graphsage.html" hidden></a>
</article>
      </div>
    </main>
    <footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-3">
        <p>Stuff I find cool/useful</p>
      </div>
    </div>
  </div>
</footer>
  </body>
</html>