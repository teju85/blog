<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/src/assets/main.css">
  
    <link rel="stylesheet" href="/src/assets/katex.min.css">
    <script rel="src" src="/src/assets/jquery-1.11.1.min.js"></script>
    <script rel="src" src="/src/assets/katex.min.js"></script>
  
  
</head>
  <body>
    <header class="site-header" role="banner">
  <div class="wrapper">
    
    
    <a class="site-title" rel="author" href="/">Quagmire</a>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
            <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
            <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
          </svg>
        </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/about.html">About</a>
        <a class="page-link" href="/allposts.html">All Posts</a>
        <a class="page-link" href="/tags.html">Tags</a>
      </div>
    </nav>
  </div>
</header>
    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">"A comprehensive survey on Graph Neural Networks"</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-06-14" itemprop="datePublished">
        Published: 2020-06-14
      </time>
      
        <br/>
        <time class="dt-published" datetime="2021-07-15" itemprop="datePublished">
          Last Modifed: 2021-07-15
        </time>
      
    </p>
  </header>

  <ul class="tags">
  
    <li><a href="/tags.html#tech-notes" class="tag">tech-notes</a></li>
  
    <li><a href="/tags.html#paper-notes" class="tag">paper-notes</a></li>
  
    <li><a href="/tags.html#graph" class="tag">graph</a></li>
  
    <li><a href="/tags.html#gnn" class="tag">gnn</a></li>
  
  </ul>

  <div class="post-content e-content" itemprop="articleBody">
    <h3>Proposal</h3>
<p>Main survey paper can be found <a href="https://arxiv.org/pdf/1901.00596.pdf">here</a>.
Divides GNNs into the following top-level four categories:</p>
<ol>
<li>recurrent GNNs - iteratively exchange info with neighbors until some form of
convergence is achieved. This idea is borrowed and improved by convGNNs.</li>
<li>convolutional GNNs - same as recGNNs, but here multiple such operations are
stacked to extract high-level node representations based on neighbors that
are further away.</li>
<li>graph autoencoders - unsupervised methods to represent graphs by latent
vectors and then use a decoder phase to reconstruct the graph</li>
<li>spatio-temporal GNNs - combine convGNNs and then regular 1D-CNNs to capture
both spatial and temporal info of the nodes, respectively</li>
</ol>
<h3>Differences wrt regular DL</h3>
<ul>
<li>data space is non-Euclidean</li>
<li>graphs can be irregular</li>
<li>number of neighbors vary, causing difficulty with regular convolutions</li>
<li>instance (aka node) is not independent of the other</li>
</ul>
<h3>Differences wrt network embedding</h3>
<ul>
<li>NE involves other non-DL methods like factorization, random-walks</li>
<li>GNNs provide and end-to-end solution for some of the graph-related tasks</li>
</ul>
<h3>recGNNs</h3>
<p>Perform message passing between nodes and their neighbors, until some form of
convergence is achieved.
$$h_v^{(t)} = \sum_{u \epsilon N(v)} f(X_v, x_{u,v}^e, x_u, h_u^{(t-1)})$$</p>
<ul>
<li>$$h_v$$ = hidden vector which is initialized to random value at time = 0</li>
<li>$$f(.)$$ = a contraction mapping parametric function to ensure convergence</li>
<li>$$X$$ = node feature matrix</li>
<li>$$x$$ = edge feature matrix</li>
<li>$$N(v)$$ = neighborhood of node $$v$$</li>
<li>n = number of nodes</li>
<li>m = number of edges</li>
</ul>
<p>Gated GNNs - use GRU with fixed number of recurrence steps, instead of a
generic contraction mapping function. Then uses BPTT (backprop through time)
in order to learn parameters.
$$h_v^{(t)} = GRU(h_v^{(t-1)}, \sum_{u \epsilon N(v)} W h_u^{(t-1)}$$</p>
<ul>
<li>$$h_v^{(0)} = x_v$$</li>
</ul>
<h3>convGNNs</h3>
<h4>spectral based</h4>
<p>Based on graph signal processing and use normalized graph laplacian
$$L = I - D^{-0.5} A D^{-0.5}$$</p>
<ul>
<li>$$D$$ = diagonal matrix with $$D_{ii} = \sum_j A_{ij}$$</li>
</ul>
<p>$$L$$ is real symmetric and positive semidefinite. So, it can be factored into
$$L = U \Lambda U^T$$</p>
<ul>
<li>$$U$$ = columwise eigenvectors ordered by eigenvalues</li>
<li>$$\Lambda$$ = diagonal matrix of eigenvalues</li>
</ul>
<p>Graph fourier transform on input $$x \epsilon R^n$$ is defined as $$y = U^T x$$
similarly vice-versa for graph inverse fourier transform.</p>
<p>Assuming a filter $$g \epsilon R^n$$, convolution operation will become
$$x*g = U (U^T x \odot U^T g) = U g_{\theta} U^T x$$ with $$g_{\theta} = U^T g$$
All spectral methods follow this, except for their choice of $$g_{\theta}$$.
Some methods are:</p>
<ol>
<li>Spectral CNNs</li>
<li>Chebyshev spectral CNNs (ChebNet)</li>
<li>CayleyNet</li>
<li>[GCN]({{ site.baseurl }}{% post_url 2020-06-09-graph-convolutional-networks %})
these are however a mix of spectral and spatial based by approximating
ChebNet at first-order and simplifying learnable parameters.</li>
</ol>
<h4>spatial based</h4>
<p>All these methods aggregate neighbor information in some form and propagate this
to the current node. Multiple such layers are stacked upon each other to
increase the neighborhood definition.</p>
<ul>
<li>Neural Network for Graphs: Somewhat resembles GCN.
$$H^{(k)} = f(X W^{(k)} + \sum_{i=1}^{k - 1} A H^{(k-1)} \Theta^{(k)})$$</li>
<li>Diffusion CNNs: $$H^{(k)} = f(W^{(k)} \odot P^k X)$$</li>
<li>Diffusion Graph Convolution: $$H = \sum_{k=0}^K f(p^k X W^{(k)})$$</li>
<li>Message Passing NN</li>
<li>GraphSage: samples the neighbors in order to avoid needing full-batch</li>
<li>fast-GCN: samples a fixed number of nodes instead of neighbors</li>
<li>Graph Attention Network
<ul>
<li>$$h_v^{(k)} = \sigma(\sum_{u \epsilon N(v) \cup v} \alpha_{vu}^{(k)} W^{(k)} h_u^{(k-1)})$$</li>
<li>$$h_v^{(0)} = x_v$$</li>
<li>$$\alpha_{vu}^{(k)} = softmax(leakyReLU(a^T [W^{(k)}h_v^{(k-1)} || W^{(k)}h_u^{(k-1)}]))$$</li>
</ul>
</li>
</ul>
<h4>spectral vs spatial</h4>
<ul>
<li>spectral methods have theoretical framework</li>
<li>spatial methods are computationally efficient and scalable</li>
<li>spatial methods allow batching of nodes in a graph</li>
<li>spectral methods only work with undirected graphs</li>
</ul>
<h4>graph pooling modules</h4>
<p>coarsen the graph representation to reduce computationaly complexity for graph
classification and such other tasks next in the pipeline. Popular approach is to
use mean/max/sum based pooling functions:
$$h_G = poolFunction(h_1^{(K)}, h_2^{(K)}, ..., h_n^{(K)})$$</p>
<h3>Graph autoencoders</h3>
<h4>network embedding</h4>
<p>Low dimensional representation of nodes which preserves the topological info.</p>
<ul>
<li>Graph AutoEncoder: uses GCN in encoder phase and decoder phase tries to
reconstruct the adjacency matrix based on the generated embedding</li>
<li>Variational GAE: Variational version of the above, using KL divergence as the
metric</li>
</ul>
<h4>graph generation</h4>
<p>Beneficial in solving molecular graph generation problem. There are methods
which try to generate graphs globally or sequentially.</p>
<h3>spatio-temporal GNNs</h3>
<p>Capture both spatial and temporal dependencies together. eg: traffic speed
forecasting. Most simple way is to feed the output of convGNNs into recurrent
layer like RNNs/LSTMs.</p>
<h3>future directions</h3>
<ol>
<li>model depth - going infinite depth will pull all nodes into a single point!
Does it make sense to increase depth?</li>
<li>scalability trade-off - use of clustering could destroy patterns, while
sampling the neighbors may miss critical info at times. Need to find a good
trade-off between scalability and info-loss</li>
<li>heterogeneous graphs support</li>
<li>dynamicity - STGNNs addresses some aspects of this.</li>
</ol>

  </div>
  <a class="u-url" href="./_posts/2020-06-14-survey-of-gnns.html" hidden></a>
</article>
      </div>
    </main>
    
  <script type="text/javascript">
   $("script[type='math/tex']").replaceWith(
     function(){
       var tex = $(this).text();
       return "<span class=\"inline-equation\">" + 
              katex.renderToString(tex) +
              "</span>";
   });
   
   $("script[type='math/tex; mode=display']").replaceWith(
     function(){
       var tex = $(this).text();
       return "<div class=\"equation\">" + 
              katex.renderToString("\\displaystyle "+tex) +
              "</div>";
   });
  </script>


<footer class="site-footer h-card">
  <data class="u-url" href="/"></data>
  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-3">
        <p>Stuff I find cool/useful</p>
      </div>
    </div>
  </div>
</footer>
  </body>
</html>