<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/src/assets/main.css">
  
    <link rel="stylesheet" href="/src/assets/katex.min.css">
    <script rel="src" src="/src/assets/jquery-1.11.1.min.js"></script>
    <script rel="src" src="/src/assets/katex.min.js"></script>
  
  
</head>
  <body>
    <header class="site-header" role="banner">
  <div class="wrapper">
    
    
    <a class="site-title" rel="author" href="/">Quagmire</a>

    
  </div>
</header>
    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">"Computational Neuroscience Coursera notes"</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-01-15" itemprop="datePublished">
        Published: 2018-01-15
      </time>
      <br/>
      <time class="dt-published" datetime="2021-07-15" itemprop="datePublished">
        Last Modifed: 2021-07-15
      </time>
    </p>
  </header>

  <ul class="tags">
  
    <li><a href="/tags.html#course-notes" class="tag">course-notes</a></li>
  
  </ul>

  <div class="post-content e-content" itemprop="articleBody">
    <p>My notes while taking the Computational Neuroscience course.</p>
<h2>Week1 - Models</h2>
<ul>
<li>description models of the brain (DM)</li>
<li>mechanistic models of brain cells and networks (MM)</li>
<li>interpretive (or normative) models of brain (IM)</li>
</ul>
<h3>what is computational neuroscience?</h3>
<ul>
<li>to explain in computational terms how brains generate behaviors</li>
<li>it provides tools and methods for characterizing what nervous systems do,
determining how they function, and understanding why operate in particular ways
<ul>
<li>DM&#x27;s explain the &#x27;what&#x27; part</li>
<li>MM&#x27;s explain the &#x27;how&#x27; part</li>
<li>IM&#x27;s explain the &#x27;why&#x27; part</li>
</ul>
</li>
</ul>
<h3>receptive fields (RFs)</h3>
<ul>
<li>specific properties of a sensory stimulus that generate a strong response from the cell</li>
<li>can describe all 3 types of models to these</li>
</ul>
<h3>DM of RFs</h3>
<ul>
<li>center-surround RFs in the retina
<ul>
<li>on-center off-surround RF (center = a small patch of retina associated with the cell)</li>
<li>off-center on-surround RF</li>
</ul>
</li>
<li>RFs can be thought of as filters giving peak response to certain types of inputs!</li>
<li>primary visual cortex = V1</li>
<li>cortical RFs - ex: oriented RFs
<ul>
<li>these oriented RFs are obtained from center-surround RFs</li>
<li>can be explained using MM</li>
</ul>
</li>
</ul>
<h3>MM of RFs</h3>
<ul>
<li>visual processing circuitry
<ul>
<li>retina -&gt; LGN -&gt; V1</li>
<li>LGN RF = center-surround</li>
<li>V1 RF = oriented RF</li>
<li>LGN = Lateral Geniculate Nucleus</li>
<li>a number of LGN cells give input to a V1 cell</li>
</ul>
</li>
<li>to get oriented RFs from center-surround, LGN cells are displaced along the preferred orientation of V1!</li>
<li>but this doesn&#x27;t take into account recurrent connection from other V1 cells</li>
</ul>
<h3>IM of RFs</h3>
<ul>
<li>efficient coding hypothesis (EC)
<ul>
<li>goal is to represent images as faithfully and efficiently as possible using neurons with RF_i</li>
<li>I = image, I&#x27; = reconstructed image, $$r_i$$ = neural responses</li>
<li>I&#x27; = $$\sum_i (RF_i * r_i)$$</li>
<li>what are the $$RF_i$$ that minimize $$|I-I&#x27;|^2$$ and are as independent as possible?</li>
</ul>
</li>
<li>start out with random $$RF_i$$ and run your EC algo on natural image patches
<ul>
<li>sparse coding</li>
<li>PCA (ICA - Independent Component Analysis)</li>
<li>predictive coding</li>
</ul>
</li>
<li>conclusion: brain may be trying to find faithful &amp; efficient representations of an animal&#x27;s natural environment</li>
</ul>
<h3>Neurobiology 101</h3>
<ul>
<li>Neuron = the brain cell (about 25um) - made up of cell-body, axon and dendrites</li>
<li>neuron doctrine
<ul>
<li>neuron is the fundamental structural &amp; functional unit of brain</li>
<li>they are discrete cells &amp; not continuous with others</li>
<li>information flows from dendrites to the axon via the cell-body</li>
</ul>
</li>
<li>idealized neuron
<ul>
<li>inputs through dendrites (Axons from other cells)</li>
<li>cell-body (with a nucleus) at the center of dendrites</li>
<li>from cell-body axons connect to presynaptic terminals (outputs)</li>
<li>axons are covered by myelin</li>
<li>at places where there is no myelin = node of ranvier</li>
</ul>
</li>
<li>input to neuron from other axons = EPSP = Excitatory Post-Synaptic Potential</li>
<li>if these inputs from other neurons exceed the threshold level, the current neuron fires an output spike
<ul>
<li>output spike = action potential</li>
</ul>
</li>
</ul>
<h4>what is a neuron?</h4>
<ul>
<li>leaky bag of charged liquid!
<ul>
<li>bag because contents of neuron are enclosed within a cell membrane</li>
<li>cell membrane is a lipid bilayer
<ul>
<li>bilayer is impermeable to charged ion species such as Na+, Cl-, K+</li>
<li>ionic channels embedded in membrane allow ions in and out</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4>electrical properties of neuron</h4>
<ul>
<li>each neuron maintains a potential difference across its membrane</li>
<li>inside is about -70mV relative to outside</li>
<li>this is the resting potential of neuron</li>
<li>this is because of higher concentration of certain ions in the outside compared to inside</li>
<li>more Na+, Cl- on outside</li>
<li>more K+, organic ions [A-] inside</li>
<li>an ionic pump maintains this potential by actively maintaining this concentration difference</li>
</ul>
<h4>ionic channels</h4>
<ul>
<li>these are made of proteins</li>
<li>and only allow certain ions to pass through them</li>
<li>these channels are gated
<ul>
<li>voltage-gated = probability of opening depends on membrane voltage</li>
<li>chemically-gated = binding to a chemical causes channel to open (ex: synapses)</li>
<li>mechanically-gated = sensitive to pressure/stretch</li>
</ul>
</li>
</ul>
<h4>neuronal signalling</h4>
<ul>
<li>these gated channels allow neuronal signalling</li>
<li>synapse = junctions between neurons</li>
<li>inputs from other neurons
-&gt; synapses opening
-&gt; change in local membrane potential
-&gt; opening/closing of voltage-gated channels in dendrites/body/axon
-&gt; causes depolarization/hyperpolarization
-&gt; strong polarization results in a spike (action potential)</li>
<li>depolarization = excess +ve voltage</li>
<li>hyperpolarization = excess -ve voltage</li>
<li>action potential flow
<ul>
<li>start  -&gt; Na+ channel open -&gt; more Na+ open    -&gt; Na+ close -&gt; K+ open -&gt; K+ close -&gt; end</li>
<li>rest. pote. -&gt; +ve V raise -&gt; more +ve V raise -&gt; peak      -&gt; V drop  -&gt; V drop   -&gt; resting potential</li>
</ul>
</li>
<li>this spike is then propagated through the axon</li>
<li>myelin due to oligodendrocytes (glial cells) wrap axons
<ul>
<li>enable fast long range spike communication!</li>
<li>these are non-conductors of electricity</li>
<li>spike hops from one node of ranvier to the next (saltatory conduction)</li>
<li>this &#x27;active wiring&#x27; allows lossless signal propagation!</li>
</ul>
</li>
</ul>
<h4>synapses</h4>
<ul>
<li>junction/connection bet. 2 neurons. 2 types:
<ul>
<li>electrical - uses gap junctions
<ul>
<li>these are fast and are used for synchronizing the firing of neurons</li>
</ul>
</li>
<li>chemical - uses neurotransmitters
<ul>
<li>one can control the effect of spike from adjacent neuron by reducing the number of receptors in the current neuron</li>
<li>these are used in memory and learning</li>
</ul>
</li>
</ul>
</li>
<li>excitatory synapse - increase postsynaptic membrane potential
<ul>
<li>input spike
-&gt; neurotransmitter release
-&gt; binds to ion channel receptors
-&gt; ion channels open
-&gt; Na+ influx
-&gt; depolarization due to excitatory postsynaptic potential (EPSP)</li>
</ul>
</li>
<li>inhibitory synapse - decrease postsynaptic membrane potential
<ul>
<li>input spike
-&gt; neurotransmitter release
-&gt; binds to ion channel receptors
-&gt; ion channels open
-&gt; K+ leaves cell
-&gt; hyperpolarization due to inhibitory postsynaptic potential (IPSP)</li>
</ul>
</li>
<li>synapse doctrine = synapses are the basis for memory and learning</li>
<li>synapse learning happens through a mechanism called synaptic plasticity (SP)
<ul>
<li>hebbian plasticity
<ul>
<li>if neuron A repeatedly takes part in firing neuron B, then their synapse should be strengthened</li>
<li>neurons that fire together, wire together</li>
<li>proof: Long Term Potentiation: LTP</li>
<li>experimentally observed increase in synaptic strength that lasts for hours/days</li>
<li>Long Term Depression: LTD</li>
<li>experimentally observed decrease in synaptic strength that lasts for hours/days</li>
</ul>
</li>
<li>SP depends on relative timing of input &amp; output spikes!
<ul>
<li>i/p spike before o/p spike =&gt; LTP</li>
<li>i/p spike after o/p spike =&gt; LTD</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4>nervous system (NS)</h4>
<ul>
<li>nerves = bundle of axons</li>
<li>Peripheral NS - PNS
<ul>
<li>somatic - nerves connecting to voluntary skeletal muscles and sensory receptors
<ul>
<li>afferent nerve fibers - incoming - axons that carry info away from PNS to CNS</li>
<li>efferent nerve fibers - outgoing - axons that carry info from CNS to PNS</li>
</ul>
</li>
<li>autonomic - nerves connecting heart, blood vessels, glands, etc.</li>
</ul>
</li>
<li>Central NS - CNS
<ul>
<li>consists of spinal cord + brain</li>
<li>spinal cord
<ul>
<li>local feedback loops control reflexes (reflex arcs)</li>
<li>descending motor control signals from brain activate spinal motor neurons</li>
<li>ascending sensory axons convey sensory info from muscles/skin back to brain</li>
</ul>
</li>
<li>brain
<ul>
<li>hindbrain
<ul>
<li>medulla oblangata - controls breathing, muscle tone, blood pressure</li>
<li>pons - connected to cerebellum, sleep and arousal</li>
<li>cerebellum - coordination &amp; timing of voluntary movements, sense of equilibrium, language, attention etc.</li>
</ul>
</li>
<li>midbrain - eye movements, visual and auditory reflexes</li>
<li>reticular formation - modulates muscle reflexes, breathing &amp; pain perception. Regulates sleep, wakefulness &amp; arousal</li>
<li>thalamus - relay station for all sensory info (except smell). regulates sleep and wakefulness</li>
<li>hypothalamus - regulates basic needs: fighting, flighting, feeding &amp; mating</li>
<li>Cerebrum - perception, motor control, memory, emotion, learning
<ul>
<li>cerebral cortex - convoluted surface of cerebrum. layered sheet of neurons
<ul>
<li>large number of neurons + synapses</li>
<li>huge amount of connections!</li>
<li>6 layers of neurons</li>
<li>relatively uniform in structure across the brain</li>
</ul>
</li>
<li>basal ganglia</li>
<li>hippocampus</li>
<li>amygdala</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>Week2 - Neural Code</h2>
<h3>recording from the brain</h3>
<ul>
<li>fMRI - function Magnetic Resonance Imaging</li>
<li>EEG - Electro EncephaloGraphy</li>
<li>Electrode Arrays - invasive, but can read from individual neurons</li>
<li>calcium imaging</li>
<li>patch electrodes - to look inside the cells</li>
<li>people also generate raster plots to look at neural activity</li>
</ul>
<h3>encoding - how does a stimulus cause a pattern of responses</h3>
<ul>
<li>building quasi mechanistic models of our neural systems</li>
<li>$$P(response|stimulus)$$ = encoding</li>
<li>typically neural response = avg. firing rate or prob. of generating a spike</li>
</ul>
<h3>decoding - what do these responses tell about the stimulus</h3>
<ul>
<li>how to reconstruct what the brain is doing</li>
<li>helps in neuro-prosthetics</li>
<li>$$P(stimulus|response)$$ = decoding</li>
</ul>
<h3>tuning curves</h3>
<ul>
<li>example: gaussian tuning curve</li>
<li>neural firing frequency over the orientation of the bar</li>
</ul>
<h3>brain seems to construct complex features from a set of lower level features</h3>
<ul>
<li>this forms basis of deep learning models!</li>
<li>however, in brain, these lower level neurons also get feedback from higher ones!</li>
</ul>
<h3>construction of response models</h3>
<ul>
<li>$$P(response|stimulus)$$ = encoding</li>
<li>r(t) = firing rate and its dependency on stimulus</li>
<li>response = spike produced by a single neuron</li>
<li>linear temporal filter: $$r(t) = \sum_k s_{t-k}*f_k$$
<ul>
<li>similar to FIR filter in DSP!</li>
<li>or a convolution filter</li>
</ul>
</li>
<li>leaky avarage (or integrator) - moving window average with weights decreasing exponentially on the past samples</li>
<li>linear spatial filter: $$r(x,y) = \sum_{x&#x27;,y&#x27;} s_{x-x&#x27;,y-y&#x27;}*f_{x&#x27;,y&#x27;}$$</li>
<li>similar logic applies to spatio-temporal filtering too
<ul>
<li>like a 3D convolution</li>
</ul>
</li>
<li>another approach could be to apply a non-linear function on the convolved output
<ul>
<li>$$r(t) = g(\sum_k s_{t-k}*f_k)$$</li>
<li>g = non-linear function, eg: sigmoid</li>
</ul>
</li>
</ul>
<h3>learning model from data</h3>
<h4>feature selection</h4>
<ul>
<li>sample (N times) the stimulus in a window to get N-dimensional vector</li>
<li>collect only those samples whose right-end-window generates a spike</li>
<li>take average of all such samples = spike-triggered average (STA)
<ul>
<li>it will be a vector in the N-dim space</li>
<li>this can also be applied to images</li>
</ul>
</li>
<li>now to apply linear filtering using STA
<ul>
<li>make this STA vector as the &#x27;f&#x27; and apply convolution with the input!</li>
<li>equivalent to vector dot product or projection</li>
</ul>
</li>
</ul>
<h4>determining the nonlinear function</h4>
<ul>
<li>this nonlinear function (or input/output function) is basically P(spike|stimulus)</li>
<li>can be found from data using Baye&#x27;s rule</li>
<li>to extend this to multiple features, think of creating a multi-feature probability distribution</li>
</ul>
<h3>Kullback-Leibler divergence</h3>
<ul>
<li>D_KL(P(s),Q(s)) = \Sigma_s P(s) * log_2(P(s)/Q(s))</li>
<li>measure of difference between 2 probability distributions</li>
<li>choose filter in order to maximize D_KL between spike-conditional and prior distributions</li>
<li>similar to maximizing mutual info between them!</li>
</ul>
<h3>neuron spike rate modelling</h3>
<ul>
<li>can use binomial distribution</li>
<li>in the limit of number of bins to inf, this distribution becomes poisson!</li>
<li>assuming each firing is independent of another, time interval between 2 adjacent spikes becomes exp. distribution</li>
<li>for higher rates, poisson looks more gaussian</li>
<li>fano factor = variance / mean</li>
<li>Generalized Linear Model
<ul>
<li>input -&gt; stimulus filter, post-spike filter
-&gt; non-linearity
-&gt; stochastic spike generator
-&gt; post-spike filter</li>
</ul>
</li>
</ul>
<h2>Week3</h2>
<h3>decoding</h3>
<ul>
<li>how well can we learn the stimulus given its neural responses</li>
<li>example: determining a threshold for classifying positive and negative samples, given their distribution
<ul>
<li>basically putting a threshold on the likelihood ratio</li>
<li>P(r|-) / P(r|+)  &gt; thresh</li>
</ul>
</li>
<li>Neyman-Pearson lemma
<ul>
<li>likelihood ratio test is the most efficient statistic in that it has the most power for a given size</li>
</ul>
</li>
<li>assuming we don&#x27;t have to decide immediately to the stimulus
<ul>
<li>keep accumulating the log of the likelihood ratio over time</li>
<li>define 2 threshold bars, one for positive and another for negative class</li>
<li>whenever the cumulative sum of this ratio reaches one of these bars, stop accumulating</li>
<li>declare the output as that class</li>
</ul>
</li>
<li>we should also be taking into account the role of priors for each of these classes!
<ul>
<li>makes sense for cases where these classes are not equally distributed</li>
</ul>
</li>
<li>should also be adding the cost of false-negatives and false-positives!</li>
</ul>
<h3>population coding and bayesian estimation</h3>
<ul>
<li>population codes = decoding from many neurons</li>
<li>cricket neurons are sensitive to wind
<ul>
<li>(f(s) / r_max) = cos(s - s_a)</li>
<li>s_a = preferred angle for the neuron where the firing rate is maximum</li>
<li>r_max = max firing rate, used to normalize the output</li>
<li>can also be written as dot product of vectors:
<ul>
<li>(f(s) / r_max) = v . c_a</li>
<li>c_a = preffered direction of wind for the neuron&#x27;s maximum firing rate</li>
</ul>
</li>
<li>summing up across all neurons gives rise to population vector</li>
</ul>
</li>
<li>however, population vector is neither general nor optimal!?</li>
<li>Baye&#x27;s law
<ul>
<li>p(s|r) = p(r|s) * p(s) / p(r)</li>
<li>p(r|s) = likelihood function, conditional distribution</li>
<li>p(s) = prior distribution</li>
<li>p(r) = marginal distribution</li>
<li>p(s|r) = aposteriori distribution</li>
</ul>
</li>
<li>assume gaussian distribution for stimulus and poisson for response</li>
<li>also assume firing rate of each neuron is independent of another</li>
<li>with this, one can generate likelihood distribution for response given stimulus
<ul>
<li>solve this to get maximum log likelihood</li>
</ul>
</li>
<li>with this, one can also generate aposteriori distribution for stimulus given response
<ul>
<li>solve this to get maximum log aposteriori</li>
</ul>
</li>
<li>but these approaches do not take into account the correlations in the population</li>
</ul>
<h3>stimulus reconstruction: (reading minds)</h3>
<ul>
<li>to create an estimator: s_bayes</li>
<li>use least squared error wrt s_bayes and s (true stimulus)</li>
</ul>
<h3>Guest Lecture Fred Reike</h3>
<ul>
<li>extracting sparse signals from noise</li>
<li>appears similar to logistic classifier!</li>
<li>Priors matter while trying to extract sparse signals from noise!</li>
<li>basically try to put the threshold based on aposteriori probability</li>
</ul>
<h2>Week4</h2>
<h3>information and entropy</h3>
<ul>
<li>probability of seeing an event = P(1) = p, thus P(0) = 1-p
<ul>
<li>information(P(1)) = -log_2(p)</li>
<li>information(P(0)) = -log_2(1-p)</li>
</ul>
</li>
<li>Entropy = avg. information = -\Sigma_i(p_i * log_2(p_i))
<ul>
<li>units are bits</li>
</ul>
</li>
<li>mutual information
<ul>
<li>amount of entropy that is used in coding the stimulus</li>
<li>MI(S,R) = TotalEntropy - AvgNoiseEntropy</li>
<li>MI(S,R) = -\Sigma_r(p(r) * log_2(p(r))) + \Sigma_s(p(s) * \Sigma_r(p(r|s) * log_2(p(r|s))))</li>
</ul>
</li>
<li>Quantifying how independent R and S are:
<ul>
<li>I(S,R) = D_KL(P(S,R), P(S)P(R))</li>
<li>using this, one can derive with the MI equation above!</li>
<li>MI is symmetric between S and R!</li>
</ul>
</li>
</ul>
<h3>calculating info in spike trains</h3>
<ul>
<li>information in spike patterns
<ul>
<li>each time, if there&#x27;s spike assign a binary 1, else 0</li>
<li>create binary words (w_i) of fixed bits out of this resulting bit-vector</li>
<li>compute p(w_i), then use entropy equation on it</li>
<li>calculate the diff between total variability driven by stimuli and that due to noise, averaged over stimuli</li>
</ul>
</li>
<li>information in single spike
<ul>
<li>repeat similar such analysis but with words of one bit!</li>
</ul>
</li>
</ul>
<h3>information and coding efficiency</h3>
<ul>
<li>natural stimuli:
<ul>
<li>typically have huge dynamic range</li>
<li>follows power-law scaling</li>
<li>have structure at many scales</li>
</ul>
</li>
<li>to have maximum output entropy, a good encoder should match its o/p&#x27;s to the distribution of its i/p&#x27;s
<ul>
<li>this also optimizes mutual info between i/p and o/p</li>
<li>and as one changes the characteristics of i/p, changes can occur in the i/p -&gt; o/p function and in the encoded feature!</li>
</ul>
</li>
<li>redundancy reduction
<ul>
<li>entropy(R_1,R_2) &lt;= entropy(R_1) + entropy(R_2)</li>
<li>R_1 and R_2 are responses of 2 neurons</li>
<li>in order to be efficient in terms of entropy and coding, neurons must be independent</li>
</ul>
</li>
<li>But neurons are observed to be redundant
<ul>
<li>error correction</li>
<li>robust coding</li>
<li>helps in discrimination</li>
</ul>
</li>
</ul>
<h2>Week5</h2>
<h3>modeling neurons</h3>
<ul>
<li>membrane can be thought of as a capacitor and resistor in parallel
<ul>
<li>thus, using kirchoff&#x27;s law:</li>
<li>C.dV/dt = -V/R + I_ext</li>
</ul>
</li>
<li>but membrane also has potential due to ionic equilibrium (called nernst potential)
<ul>
<li>E = k_B * T / (z * q) * ln(inside/outside)
<ul>
<li>k_B = boltzmann constant</li>
<li>inside, outside = ionic concentration</li>
<li>T = temperature</li>
<li>z = number of charges in the ion</li>
<li>q = ionic charge</li>
</ul>
</li>
<li>thus, part of voltage drop across resistor will be due to this potential</li>
<li>C.dV/dt = -(V - V_rest)/R + I_ext</li>
<li>solution to this is the classic exponential decay RC circuit!</li>
</ul>
</li>
<li>conductance = resistance^-1</li>
<li>different ion channels have associated conductances
<ul>
<li>given conductance tends to move the membrane potential toward the equilibrium potential for that ion</li>
</ul>
</li>
</ul>
<h3>spikes (what makes a neuron compute)</h3>
<ul>
<li>excitability arises from ion channel nonlinearity</li>
<li>the conductances of these channels depend on the voltage difference!</li>
<li>hodgkin huxley equation describes the movement of Na/K ions through the membrane</li>
</ul>
<h3>simplified model neurons</h3>
<ul>
<li>motor neurons typically fire in regular intervals</li>
<li>integrate-and-fire neuron model
<ul>
<li>pretty close to real neuronal spikes</li>
<li>V_0 = stability potential</li>
<li>V_reset = reset potential immediately after the spike</li>
<li>V_th = threshold potential after which spike occurs</li>
<li>V_max = max spike potential</li>
</ul>
</li>
<li>to make the above model fire intrinsically
<ul>
<li>add a non-linearity after the linear region
<ul>
<li>quadratic or exponential</li>
<li>remaining else same as before</li>
</ul>
</li>
</ul>
</li>
<li>another model is theta neuron
<ul>
<li>to model periodically firing neurons</li>
</ul>
</li>
<li>for 2-dimensional models, using nullclines, one can determine the stable point</li>
</ul>
<h3>forest of dendrites</h3>
<ul>
<li>neurons have complicated spatial structures</li>
<li>voltage decays with distance in passive membranes</li>
<li>this is modelled use cable theory (or cable equation)</li>
<li>rall model for passive dendrites</li>
</ul>
<h3>Guest Lecture Eric Shea Brown</h3>
<ul>
<li>correlations and synchrony in neural populations</li>
<li>tuning curve = plot of firing rate against the stimulus</li>
<li>pairwise correlation
<ul>
<li>count number of spikes in a moving window for 2 neurons</li>
<li>\rho = cov(n_1,n_2) / sqrt(var(n_1)*var(n_2))</li>
</ul>
</li>
<li>correlation can degrade the signal encoding
<ul>
<li>it becomes hard to distinguish between the responses for different stimuli</li>
<li>since there will be more overlap of responses</li>
</ul>
</li>
<li>more number of neurons we have, the better the SNR is (mean:std ratio)
<ul>
<li>if they are all statistically independent, the SNR just keeps growing</li>
<li>however, if there&#x27;s some correlation between them, the SNR saturates after certain number of neurons!</li>
<li>more the correlation, the lesser the value at which SNR saturates</li>
</ul>
</li>
<li>but in other cases of negative correlation, it can enhance the signal encoding too!</li>
<li>thus, information increases in heterogeneous populations</li>
<li>information decreases in homogeneous populations</li>
</ul>
<h2>Week6</h2>
<h3>modeling synapses and network of connected neurons</h3>
<ul>
<li>chemical synapses are the most common ones found in the brain</li>
<li>modeling a synapse is through an RC circuit</li>
<li>modeling synapse inputs to a post-synaptic neuron is again through another channel of conductance
<ul>
<li>this synaptic conductance however changes based on the spikes ariving in the pre-synaptic neuron</li>
</ul>
</li>
<li>simple model of 2 neurons connected to each other
<ul>
<li>both are excitatory = alternate firing</li>
<li>both are inhibitory = synchrony!</li>
</ul>
</li>
</ul>
<h3>network models</h3>
<ul>
<li>modelling based on spiking neurons
<ul>
<li>computationally expensive</li>
<li>can model synchrony between neurons</li>
</ul>
</li>
<li>modelling based on firing rate
<ul>
<li>more efficient representation and modelling large networks</li>
<li>no spike timing issues</li>
</ul>
</li>
<li>spiking neuron model = linear filter model on the firing rate model!
<ul>
<li>we can do so only when the neurons are not correlated to each other</li>
<li>spike train \rho_b(t) can be replaced with firing rate u_b(t)</li>
</ul>
</li>
<li>total synaptic current to the neuron input is summation of weighted inputs from all its synapses</li>
<li>firing rate based model
<ul>
<li>output firing rate changes like this
<ul>
<li>\tau_r . dv/dt = -v + F(I_s(t))</li>
</ul>
</li>
<li>input synaptic current changes like this
<ul>
<li>\tau_s . dI_s/dt = -I_s + w.u</li>
</ul>
</li>
<li>the steady state function is the same as used in ANNs!</li>
<li>recurrent networks
<ul>
<li>\tau . dv/dt = -v + F(Wu + Mv)</li>
<li>M = weight matrix for recurrent connections</li>
<li>W = weight matrix for feedforward connections</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>recurrent networks</h3>
<ul>
<li>linear recurrent network with symmetric M
<ul>
<li>will give orthogonal eigen vectors</li>
<li>thus we can write output to be linear combination of these eigen vectors</li>
<li>v(t) = \Sigma_i c_i(t)e_i</li>
<li>then we can solve for each of these c_i to finally solve for v(t)</li>
<li>the eigen values determine the network stability</li>
<li>it can amplify its inputs!</li>
<li>it can also manifest memory!</li>
</ul>
</li>
<li>non-linear recurrent networks with symmetric M
<ul>
<li>these can also amplify its inputs!</li>
<li>even if eigen value is greater than 1, it can be stable, due to rectifier non-linearity!
<ul>
<li>this would have caused instability in case of linear recurrent networks</li>
</ul>
</li>
<li>can also perform &#x27;selective attention&#x27;!
<ul>
<li>performs &#x27;winner-takes-all&#x27; input selection</li>
</ul>
</li>
<li>can perform gain modulation</li>
<li>can also maintain memory</li>
</ul>
</li>
<li>non-linear recurrent networks with non-symmetric M</li>
</ul>
<h2>Week7</h2>
<h3>synaptic plasticity and learning</h3>
<ul>
<li>long term potentiation (LTP)
<ul>
<li>one type of plasticity</li>
<li>experimentally observed increase in synaptic strength that lasts for hours or days</li>
</ul>
</li>
<li>long term depression (LTD)
<ul>
<li>experimentally observed decrease in synaptic strength that lasts for hours or days</li>
</ul>
</li>
<li>Hebbian learning rule
<ul>
<li>if neuron A repeatedly takes part in firing neuron B, then synapse from A to B is strengthened</li>
</ul>
</li>
<li>formalizing Hebbian learning rule will give us the standard gradient descent equation!</li>
<li>Hebb&#x27;s rule increases only LTP, but not LTD
<ul>
<li>covariance rule helps solve this</li>
</ul>
</li>
<li>however, for both hebb and covariance rules, the length of weight vector grows unbounded over time!</li>
<li>Oja&#x27;s rule for hebbian learning - stable and converges to a value</li>
<li>for solving hebbian equation using covariance rule, we can use eigen vectors!
<ul>
<li>this means, that brain is basically performing Principal Component Analysis!!! (PCA)</li>
</ul>
</li>
</ul>
<h3>Unsupervised learning</h3>
<ul>
<li>competitive learning
<ul>
<li>given a new input, pick the most active neuron (winner takes all)</li>
<li>update weight vector for that neuron</li>
</ul>
</li>
<li>Self Organizing Maps (Kohonen Maps)
<ul>
<li>given a new input, pick the winning neuron</li>
<li>update weights for it and its neighbors</li>
<li>eg: orientation preference map in the primary visual cortex</li>
</ul>
</li>
<li>goal of unsupervised learning
<ul>
<li>learn a generative model for the data being observed</li>
<li>ie. mimic the data generation proces</li>
<li>it involves 2 steps:
<ol>
<li>learning the posterior probability for the new input</li>
<li>using this probablity to learn the parameters</li>
</ol>
</li>
</ul>
</li>
<li>EM algo for unsupervised learning
<ul>
<li>involves 2 steps: E-step and M-step</li>
<li>which pretty much matches the above 2 steps mentioned!</li>
<li>competitive learning is online, whereas EM requires all the data points to be available!</li>
</ul>
</li>
</ul>
<h3>Sparse coding and predictive coding</h3>
<ul>
<li>how to learn good representation for images?</li>
<li>eigenvectors?
<ul>
<li>eg: eigenfaces!</li>
<li>eigenvector representation is good for compression, but not so good if you want to
extract the local components of an image!</li>
</ul>
</li>
<li>linear model of natural images?
<ul>
<li>in this case, we need to learn the basis vector for all the components and their weights</li>
<li>for this we can learn a generative model for the input image</li>
<li>assume parse distribution for the weights</li>
<li>will get an equation that is close to the cost function with regularization error!</li>
<li>when you try to maximize the value for &#x27;v&#x27;, you get recurrent network!</li>
<li>when you try to maximize the value for &#x27;G&#x27;, you get hebbian rule!</li>
</ul>
</li>
</ul>
<h2>Week8</h2>
<h3>supervised learning</h3>
<ul>
<li>simplest model is of perceptron
<ul>
<li>they can perform linear classification</li>
<li>based on the output error, weights are updated accordingly</li>
<li>but perceptrons cannot learn any function
<ul>
<li>example: XOR function!</li>
</ul>
</li>
<li>they can only classify linearly separable data</li>
</ul>
</li>
<li>can use multilayer perceptron to classify linearly inseparable data</li>
<li>for continuous valued output, use sigmoid function in place of step function</li>
<li>learning multilayered sigmoid networks
<ul>
<li>use gradient descent to learn the weight matrices</li>
<li>perform the same + backpropagation learning rule to learn the weights in lower layers</li>
</ul>
</li>
</ul>
<h3>reinforcement learning: predicting rewards</h3>
<ul>
<li>selecting an action that maximizes the total expected future reward</li>
<li>predicting delayed rewards
<ul>
<li>predict rewards delivered some time after the stimulus is presented</li>
<li>use a tapped delay line and weighted summation of past inputs</li>
<li>the approach is very similar to creating a discrete linear filter!</li>
<li>to learn weights, minimize the predicted reward error</li>
<li>however, we have future rewards that are not yet available!</li>
</ul>
</li>
<li>Temporal Difference (TD) learning
<ul>
<li>idea is to rewrite the error function to get rid of future terms</li>
<li>minimize this using gradient descent</li>
</ul>
</li>
</ul>
<h3>reinforcement learning: selecting action</h3>
<ul>
<li>learning a state-to-action mapping (aka policy)</li>
<li>actor-critic learning
<ul>
<li>actor = selects action and maintains policy</li>
<li>critic = maintains the value of each state</li>
</ul>
</li>
<li>learning process
<ol>
<li>critic learning (policy evaluation): using TD rule</li>
<li>actor learning (policy improvement): select an action at a given state using softmax function</li>
<li>Repeat 1 and 2</li>
</ol>
</li>
</ul>

  </div>
  <a class="u-url" href="./_posts/2018-01-15-computational-neuroscience.html" hidden></a>
</article>
      </div>
    </main>
    
  <script type="text/javascript">
   $("script[type='math/tex']").replaceWith(
     function(){
       var tex = $(this).text();
       return "<span class=\"inline-equation\">" + 
              katex.renderToString(tex) +
              "</span>";
   });
   
   $("script[type='math/tex; mode=display']").replaceWith(
     function(){
       var tex = $(this).text();
       return "<div class=\"equation\">" + 
              katex.renderToString("\\displaystyle "+tex) +
              "</div>";
   });
  </script>
      

<footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-3">
        <p>Stuff I find cool/useful</p>
      </div>
    </div>
  </div>
</footer>
  </body>
</html>